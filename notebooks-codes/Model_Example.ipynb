{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Esempio di Modello con Hugging Face\n",
    "\n",
    "## üìå Obiettivo\n",
    "\n",
    "Esplorare l‚Äô**architettura di un transformer reale** e generare testo tramite **Hugging Face Transformers**, comprendendo ogni fase del processo:\n",
    "\n",
    "---\n",
    "\n",
    "## üß∞ Setup Iniziale\n",
    "\n",
    "1. **Caricamento del modello**:\n",
    "   - Si utilizza il modello `Phi-3-mini` e il suo **tokenizer**.\n",
    "   - L'esecuzione √® su **CPU**, quindi √® normale vedere warning legati alle performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\felip\\Desktop\\How Transformer LLMs Work\\notebooks-codes\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\felip\\Desktop\\How Transformer LLMs Work\\notebooks-codes\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  3.43it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parametri di generazione**:\n",
    "   - `max_new_tokens=50`: genera al massimo 50 token.\n",
    "   - `do_sample=False`: attiva **greedy decoding** (equivalente a `temperature=0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline \n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=50, # ogni volta che diamo un prompt vogliamo che si generino al massimo 50 token\n",
    "    do_sample=False # significa che stiamo usando il greedy decoding, ciascun token avr√† la probabilit√† pi√π alta di essere selezionato (come assegnare temperature=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Esecuzione: Prompt + Output\n",
    "\n",
    "**Prompt di esempio**:\n",
    "```text\n",
    "Write an email apologizing to Sarah for the tragic gardening mishap.\n",
    "```\n",
    "\n",
    "Il modello genera un'email fino al 50¬∞ token. Puoi cambiare il prompt e osservare come il modello si comporta.\n",
    "\n",
    "üñ•Ô∏è L'esecuzione su CPU richiede pi√π tempo (~2 min).\n",
    "IL quale √® il motivo per cui nell'industria √® pi√π comune l'uso di GPU dove i modelli vengono altamente ottimizzati, e molti di questi medotodi di efficienza che abbiamo discusso sono importanti per velocizzare le generazioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sarah,\n",
      "\n",
      "I am deeply sorry for the unfortunate incident that occurred in your garden. I understand how much effort and love you put into maintaining it, and I feel terrible for the damage that was caused.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write an email apoligizing to Sarah for the traguc gardering mishap. Explain how it happened.\"\n",
    "\n",
    "output = generator(prompt)\n",
    "\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß± Anatomia del Modello\n",
    "\n",
    "Stampando `model` si osserva la **architettura** del modello:\n",
    "\n",
    "- **Tipo**: modello autoregressivo (causal language modeling), ci√≤ significa che lo step di attention considera solo i token precedenti.\n",
    "- **Architettura**:\n",
    "  - 32 transformer **decoder layers**\n",
    "  - **Dimensionalit√† vettoriale**: 3.072\n",
    "  - **Feedforward dimension**: 16.000\n",
    "  - **Vocabolario**: 32.064 token\n",
    "  - **Componenti**:\n",
    "    - Self-attention\n",
    "    - Rotary embeddings\n",
    "    - MLP (Multi-Layer Perceptron ovvero Feedforward network)\n",
    "    - Activation + LayerNorm\n",
    "    - Language modeling head (la fine del modello che prende in input il vettore finale di dimensioni 3.072 e da in output uno score per ciascuno dei token che il modello ha nel suo vocabolario ovvero 32.064 scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm()\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3Model(\n",
       "  (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "  (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x Phi3DecoderLayer(\n",
       "      (self_attn): Phi3Attention(\n",
       "        (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "        (rotary_emb): Phi3RotaryEmbedding()\n",
       "      )\n",
       "      (mlp): Phi3MLP(\n",
       "        (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "        (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        (activation_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): Phi3RMSNorm()\n",
       "      (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (post_attention_layernorm): Phi3RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): Phi3RMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3DecoderLayer(\n",
       "  (self_attn): Phi3Attention(\n",
       "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "    (rotary_emb): Phi3RotaryEmbedding()\n",
       "  )\n",
       "  (mlp): Phi3MLP(\n",
       "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "    (activation_fn): SiLU()\n",
       "  )\n",
       "  (input_layernorm): Phi3RMSNorm()\n",
       "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (post_attention_layernorm): Phi3RMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accediamo al primo transformer block\n",
    "model.model.layers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Tokenizzazione e Decoding\n",
    "\n",
    "1. **Tokenizzazione**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 450, 7483,  310, 3444,  338]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"The capital of France is\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Passaggio attraverso il modello (senza head)**:\n",
    "\n",
    "Notare che inviamo gli ids al componente `model` del modello.\n",
    "Quindi questo non passa attraverso la head di language modeling.\n",
    "Fluisce attraverso lo stack dei transformer blocks.\n",
    "\n",
    "![alt text](\"../images/compo.png\")\n",
    "\n",
    "Vediamo che l'otput √® un tensor di dimensione `(1, num_tokens, 3072)`\n",
    "\n",
    "Dove 1 indica il numero di batch, abbiamo passato al modello solo un testo, invece surante l'addestramento si hanno molti batch che contentono molti testi.\n",
    "\n",
    "5 √® il numero di token che abbiamo passato al modello, mentre 3072 √® la dimensione dei vettori di output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 3072])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inviamo gli inputs_isd al modello\n",
    "model_output = model.model(input_ids)\n",
    "model_output[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Passaggio alla head di language modeling**:\n",
    "\n",
    "Prende in input vettori di dimensione 3072 e da in output gli score per ciascuno dei token del vocabolario del tokenizer ovvero 32064.\n",
    "\n",
    "Il token con lo score/probabilit√† pi√π alto, dei 32064 sar√† il token che si da in output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 32064])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head_output = model.lm_head(model_output[0])\n",
    "lm_head_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Predizione del prossimo token**:\n",
    "\n",
    "Per ottenere il primo token di output del modello per questo prompt prendiamo l'ultimo token della sequenza, che sar√† il token di output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([27.8750, 29.5000, 28.1250,  ..., 20.5000, 20.5000, 20.5000],\n",
       "       dtype=torch.bfloat16, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_id = lm_head_output[0, -1]\n",
    "token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32064])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_id.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "token_id = lm_head_output[0, -1].argmax(-1)\n",
    "\n",
    "lm_head_output[0, -1] prende l'ultima posizione della sequenza (-1) nel primo esempio del batch (0).\n",
    "\n",
    "√à un vettore di logits di dimensione vocab_size.\n",
    "\n",
    "argmax(-1) prende l‚Äôindice (cio√® l‚ÄôID del token) con il logit pi√π alto ‚áí il token che il modello sceglierebbe se stesse facendo greedy decoding.\n",
    "\n",
    "‚úÖ Quindi, questa riga seleziona il token che il modello vuole generare dopo l‚Äôultima posizione della sequenza.\n",
    "\n",
    "‚úÖ Quindi‚Ä¶ cosa fa argmax(-1)?\n",
    "-1 √® l‚Äôultima dimensione del tensore, cio√®:\n",
    "\n",
    "Se hai un tensore 2D di shape (sequence_length, vocab_size),\n",
    "allora argmax(-1) cerca il valore massimo per ogni riga (cio√® per ogni posizione nella sequenza) lungo la dimensione dei token del vocabolario.\n",
    "Se il tensore √® 1D trova il valore massimo del vettore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3681)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_id = token_id.argmax(-1)\n",
    "# token con la probabilit√† pi√π alta\n",
    "token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úîÔ∏è Il token predetto per \"The capital of France is\" ‚Üí `\"Paris\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decodifichiamo tale token_id \n",
    "tokenizer.decode(token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Conclusioni\n",
    "\n",
    "Il modello non vede mai il testo dato in input (prompt). \n",
    "Il modello vede solamente questa lista di numeri (token_ids), e da in output altre liste di numeri (ids dei token previsti che vengono decodificati col tokenizer). \n",
    "\n",
    "Tutto ci√≤ che il modello tocca sono solo gli ids dei token e i loro embeddings. Non vede alcuna lettera del testo che diamo in input.\n",
    "\n",
    "Questo mostra quanto affascinante √® il modo in cui i language models operano senza mai vedere veramente il linguaggio umano.\n",
    "\n",
    "Questo esempio dimostra:\n",
    "- Come funziona la **generazione autoregressiva**\n",
    "- Come **token**, **matrici**, e **proiezioni** si integrano nei Transformer\n",
    "- Come accedere al modello per ispezionarlo in profondit√†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
