{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d110298-6a58-4338-bc06-074bacc3020d",
   "metadata": {},
   "source": [
    "# Tokenizers\n",
    "\n",
    "Compariamo alcuni LLM tokenizers pre-addestrati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4b054ad-bdbe-4d45-9ae3-ce52124e419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers>=4.46.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23d77b7-7972-41a6-bce2-52621ff8cf8d",
   "metadata": {},
   "source": [
    "## Tokenizing Text\n",
    "\n",
    "Tokenizzeremo la frase \"Hello world!\" usando il tokenizer del modello **bert-base-cased**.\n",
    "\n",
    "Importiamo la classe **Autotokenizer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bc6c6b6-b5b6-47a9-80bb-e7dcf79aedcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96f0163d-57b5-431d-8784-60a05e50e074",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Hello world!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76f5a5a9-8fa3-409a-9524-51b126f0bb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carichiamo il tokenizer pre-addestrato\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc2426b-7334-43ce-a102-0b141be08fa9",
   "metadata": {},
   "source": [
    "Applichiamo il tokenizer alla frase. Il tokenizer splitta la frase in tokens and returns the IDs of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32538732-61e1-4a69-afcc-a85e163bda1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer(sentence).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7376ae3b-d309-485d-a0d9-a4e22a532ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 8667, 1362, 106, 102]\n"
     ]
    }
   ],
   "source": [
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e198bc-670b-44f4-8f3e-fabb025ca25b",
   "metadata": {},
   "source": [
    "Per mappare ciascun token ID al suo corrispondente token, possiamo usare il metodo **decode** del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df183722-607d-4c84-b3ed-8ba40da61d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      "Hello\n",
      "world\n",
      "!\n",
      "[SEP]\n"
     ]
    }
   ],
   "source": [
    "for id in token_ids:\n",
    "    print(tokenizer.decode(id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c18ae7d-ba0d-420c-b79a-14975cfcab6a",
   "metadata": {},
   "source": [
    "## Visualizziamo la tokenizzazione\n",
    "\n",
    "Involucriamo gli step precedenti dentro una funzione **show_tokens**.\n",
    "La funzione prende in input un testo e il nome del modello, e stampa la lunghezza del vocabolario del tokenizer e una lista colorata dei tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e78d4e0-26b8-4955-bf75-1e65da507958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una lista di colori in RGB per evidenziare i tokens e ci aiuta a differenziare tokens diversi\n",
    "colors = [\n",
    "    \"102;194;165\", \"252;141;98\", \"141;160;203\",\n",
    "    \"231;138;195\", \"166;216;84\", \"255;217;47\"\n",
    "]\n",
    "\n",
    "def show_tokens(sentence: str, tokenizer_name: str):\n",
    "    \"\"\"Show the tokens each separated by a different color.\"\"\"\n",
    "\n",
    "    # carichiamo il tokenizer e tokenizziamo l'input\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    token_ids = tokenizer(sentence).input_ids\n",
    "\n",
    "    # stampiamo la lunghezza del vocabolario\n",
    "    print(f\"Vocab length: {len(tokenizer)}\")\n",
    "\n",
    "    # stampa una lista colorata di tokens\n",
    "    for idx, t in enumerate(token_ids):\n",
    "        print(\n",
    "            f\"\\x1b[0;30;48;2;{colors[idx % len(colors)]}m\" +\n",
    "            tokenizer.decode(t) + \n",
    "            \"\\x1b[0m\",\n",
    "            end=\" \"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d710e7ef-eb29-4635-9c46-72b5223bb445",
   "metadata": {},
   "source": [
    "Qui vedimao il testo che useremo per esplorare le diverse strategie di tokenizzazione di ciascun modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02cc8b09-86ee-42ff-b9eb-8d85afd1ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "English and CAPITALIZATION\n",
    "üéµ È∏ü\n",
    "show_tokens False None elif == >= else: two tabs:\"    \" Three tabs: \"      \"\n",
    "12.0*50=600\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e226469c-f012-48ee-9dfc-f65c4aefe18e",
   "metadata": {},
   "source": [
    "Ora usiamo di nuovo il tokenizer del modello bert-base-cased e compariamo la sua strategia di tokenizzazione a quella di **Xenova/gpt-4** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b74f06fe-cca4-43d9-b67a-17a665875102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length: 28996\n",
      "\u001b[0;30;48;2;102;194;165m[CLS]\u001b[0m \u001b[0;30;48;2;252;141;98mEnglish\u001b[0m \u001b[0;30;48;2;141;160;203mand\u001b[0m \u001b[0;30;48;2;231;138;195mCA\u001b[0m \u001b[0;30;48;2;166;216;84m##PI\u001b[0m \u001b[0;30;48;2;255;217;47m##TA\u001b[0m \u001b[0;30;48;2;102;194;165m##L\u001b[0m \u001b[0;30;48;2;252;141;98m##I\u001b[0m \u001b[0;30;48;2;141;160;203m##Z\u001b[0m \u001b[0;30;48;2;231;138;195m##AT\u001b[0m \u001b[0;30;48;2;166;216;84m##ION\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m \u001b[0;30;48;2;252;141;98mshow\u001b[0m \u001b[0;30;48;2;141;160;203m_\u001b[0m \u001b[0;30;48;2;231;138;195mtoken\u001b[0m \u001b[0;30;48;2;166;216;84m##s\u001b[0m \u001b[0;30;48;2;255;217;47mF\u001b[0m \u001b[0;30;48;2;102;194;165m##als\u001b[0m \u001b[0;30;48;2;252;141;98m##e\u001b[0m \u001b[0;30;48;2;141;160;203mNone\u001b[0m \u001b[0;30;48;2;231;138;195mel\u001b[0m \u001b[0;30;48;2;166;216;84m##if\u001b[0m \u001b[0;30;48;2;255;217;47m=\u001b[0m \u001b[0;30;48;2;102;194;165m=\u001b[0m \u001b[0;30;48;2;252;141;98m>\u001b[0m \u001b[0;30;48;2;141;160;203m=\u001b[0m \u001b[0;30;48;2;231;138;195melse\u001b[0m \u001b[0;30;48;2;166;216;84m:\u001b[0m \u001b[0;30;48;2;255;217;47mtwo\u001b[0m \u001b[0;30;48;2;102;194;165mta\u001b[0m \u001b[0;30;48;2;252;141;98m##bs\u001b[0m \u001b[0;30;48;2;141;160;203m:\u001b[0m \u001b[0;30;48;2;231;138;195m\"\u001b[0m \u001b[0;30;48;2;166;216;84m\"\u001b[0m \u001b[0;30;48;2;255;217;47mThree\u001b[0m \u001b[0;30;48;2;102;194;165mta\u001b[0m \u001b[0;30;48;2;252;141;98m##bs\u001b[0m \u001b[0;30;48;2;141;160;203m:\u001b[0m \u001b[0;30;48;2;231;138;195m\"\u001b[0m \u001b[0;30;48;2;166;216;84m\"\u001b[0m \u001b[0;30;48;2;255;217;47m12\u001b[0m \u001b[0;30;48;2;102;194;165m.\u001b[0m \u001b[0;30;48;2;252;141;98m0\u001b[0m \u001b[0;30;48;2;141;160;203m*\u001b[0m \u001b[0;30;48;2;231;138;195m50\u001b[0m \u001b[0;30;48;2;166;216;84m=\u001b[0m \u001b[0;30;48;2;255;217;47m600\u001b[0m \u001b[0;30;48;2;102;194;165m[SEP]\u001b[0m "
     ]
    }
   ],
   "source": [
    "# bert-base-cased\n",
    "show_tokens(text, \"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e3fe45-0fe4-4917-aac7-b1b536579577",
   "metadata": {},
   "source": [
    "Vediamo che il tokenizer del modello bert-base-cased ha difficolt√† nel rappresentare la parola CAPITALIZATION. Infatti notiamo come viene splittata in pi√π tokens. Gli hashtag mostrano che tali tokens appartengono al token precedente e che assieme essi rappresentano una singola parola. Il token **UNK** rapprsenta il token Unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6a00349-2160-404f-89f1-87b92a17defa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3713b03749e244a3ba768977c109489d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/460 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0321ae39a50b441db8f1fbdfea907685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.01M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32448f77c4f04e2f8b27521a3109c9da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/917k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae26c3ca29b4ba7b5e6f4fc72869b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/4.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c16bfbcd1f49bd86fe9a679847aecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/98.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length: 100263\n",
      "\u001b[0;30;48;2;102;194;165m\n",
      "\u001b[0m \u001b[0;30;48;2;252;141;98mEnglish\u001b[0m \u001b[0;30;48;2;141;160;203m and\u001b[0m \u001b[0;30;48;2;231;138;195m CAPITAL\u001b[0m \u001b[0;30;48;2;166;216;84mIZATION\u001b[0m \u001b[0;30;48;2;255;217;47m\n",
      "\u001b[0m \u001b[0;30;48;2;102;194;165mÔøΩ\u001b[0m \u001b[0;30;48;2;252;141;98mÔøΩ\u001b[0m \u001b[0;30;48;2;141;160;203mÔøΩ\u001b[0m \u001b[0;30;48;2;231;138;195m ÔøΩ\u001b[0m \u001b[0;30;48;2;166;216;84mÔøΩ\u001b[0m \u001b[0;30;48;2;255;217;47mÔøΩ\u001b[0m \u001b[0;30;48;2;102;194;165m\n",
      "\u001b[0m \u001b[0;30;48;2;252;141;98mshow\u001b[0m \u001b[0;30;48;2;141;160;203m_tokens\u001b[0m \u001b[0;30;48;2;231;138;195m False\u001b[0m \u001b[0;30;48;2;166;216;84m None\u001b[0m \u001b[0;30;48;2;255;217;47m elif\u001b[0m \u001b[0;30;48;2;102;194;165m ==\u001b[0m \u001b[0;30;48;2;252;141;98m >=\u001b[0m \u001b[0;30;48;2;141;160;203m else\u001b[0m \u001b[0;30;48;2;231;138;195m:\u001b[0m \u001b[0;30;48;2;166;216;84m two\u001b[0m \u001b[0;30;48;2;255;217;47m tabs\u001b[0m \u001b[0;30;48;2;102;194;165m:\"\u001b[0m \u001b[0;30;48;2;252;141;98m   \u001b[0m \u001b[0;30;48;2;141;160;203m \"\u001b[0m \u001b[0;30;48;2;231;138;195m Three\u001b[0m \u001b[0;30;48;2;166;216;84m tabs\u001b[0m \u001b[0;30;48;2;255;217;47m:\u001b[0m \u001b[0;30;48;2;102;194;165m \"\u001b[0m \u001b[0;30;48;2;252;141;98m     \u001b[0m \u001b[0;30;48;2;141;160;203m \"\n",
      "\u001b[0m \u001b[0;30;48;2;231;138;195m12\u001b[0m \u001b[0;30;48;2;166;216;84m.\u001b[0m \u001b[0;30;48;2;255;217;47m0\u001b[0m \u001b[0;30;48;2;102;194;165m*\u001b[0m \u001b[0;30;48;2;252;141;98m50\u001b[0m \u001b[0;30;48;2;141;160;203m=\u001b[0m \u001b[0;30;48;2;231;138;195m600\u001b[0m \u001b[0;30;48;2;166;216;84m\n",
      "\u001b[0m "
     ]
    }
   ],
   "source": [
    "# Usiamo il tokenizer del modello GPT-4, chiamato Tiktoken\n",
    "show_tokens(text, \"Xenova/gpt-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9d5d48-e08b-4414-83de-a85ab0a710b1",
   "metadata": {},
   "source": [
    "Vediamo che il tokenizer Tiktoken del modello gpt-4 √® molto pi√π grande del tokenizer del modello bert-base-cased con una lunghezza di 100263 tokens. Tiktoken necessita di meno tokens per rappresentare gli inputs. Vediamo come la parola CAPITALIZATION ora √® stata splittata in soli due tokens. √à interessante vedere come necessita meno tokens per rappresentare gli inputs, anche i tabs gli rapprsenta molto meglio. DAto che ha un vocabolario molto pi√π grande √® pi√π semplice rappresentare tokens meno comuni, ma per√≤ c'√® un trade-off. Pi√π grande √® il vocabolario, pi√π embeddings devono essere calcolati per ciascuno di questi tokens. Quindi c'√® un trade-off tra scegliere un tokenizer con potenzialmente una lunghezza di milioni di tokens, e imparare la rappresentazione (embeddings) per ciascuno di questi tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307a8b34-4bb5-4090-bf48-46d0da0d0a17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
