##"How Transformer LLMs Work"

Questo mini corso offre una panoramica approfondita sull'architettura dei modelli di linguaggio di grandi dimensioni (LLM) basati su Transformer, coprendo i seguenti argomenti:

1. **Introduzione ai Modelli di Linguaggio**: Concetti fondamentali come il modello "bag-of-words" e le word embeddings.

2. **Meccanismi di Attenzione**: Spiegazione del self-attention e del multi-head attention, elementi chiave dei Transformer.

3. **Architettura dei Transformer**: Analisi dei componenti principali, inclusi encoder e decoder, con focus sul Transformer block.

4. **Tokenizzazione**: Importanza della tokenizzazione e esempi pratici di implementazione.

5. **Miglioramenti Recenti**: Discussione su innovazioni come il Mixture of Experts (MoE) e altre ottimizzazioni nell'architettura dei Transformer.
