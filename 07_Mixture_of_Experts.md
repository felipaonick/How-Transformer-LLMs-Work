# üß† Mixture of Experts (MoE): Funzionamento, Architettura e Vantaggi

---

## üîç Cos'√® il Mixture of Experts?

Il **Mixture of Experts (MoE)** √® una tecnica avanzata che estende l'architettura dei Transformer introducendo **"esperti" dinamici**.

L'obiettivo √®:
- Migliorare l'efficienza computazionale
- Aumentare la qualit√† della generazione
- Ridurre la ridondanza del calcolo

---

## üß± Struttura base di un decoder Transformer

1. **Input**: vettori dei token in input
2. **Layer Normalization**
3. **Self-Attention mascherata**
    - Applichiamo la self-attention masked agli inputs per pesare i token in base alla loro importanza relativa al contesto di tutti gli altri token.
    - Questo output √® aggregato assieme con gli inputs non processati creando sia un direct che un indirect path.
    - Questo vonclude la parte pi√π importante dei transformer models (attention mechanism).
    - Il meccanismo di attentione prepara gli input in tal modo che pi√π informazione contestuale viene immagazzinata nei vettori. 
4. **Layer Normalization**
    - I vettori uscenti dal meccanismo di attenzione aggregati assieme ai vettori non processati sono poi normalizzati prima di essere processati da una **feed-forward neural network**.
5. **Feed-Forward Neural Network (FFN)**
    - Tale rete √® tipicamente una dei pi√π grandi componenti di un LLM, poich√® il suo scopo √® trovare relazioni complesse nell'informazione processata dal meccanismo di attenzione.
    - La rete feedforward prende gli input e li processa attraverso uno o pi√π hidden layers, questa √® chiamata dense network dato che tutti i paramentri della rete sono attivati e usati.

![alt text](images/dense.png)


6. **Output**

Nella versione classica, il FFN √® **denso**: ogni token attiva **tutti** i parametri.

---

## üîÑ Introduzione del Mixture of Experts

Il MoE **sostituisce il feed-forward network singolo** con:
- ‚úÖ **Pi√π reti neurali** (chiamate *experts*)
- ‚úÖ Un **router** che seleziona quale expert usare per ogni token

> Questo crea un **modello sparso**: solo alcuni esperti vengono attivati per ogni input.

---

## üß† Gli esperti (Experts)

- Ogni *expert* √® una rete neurale feed-forward.
- Gli esperti **non sono specializzati per dominio tematico**, ma per pattern lessicali (es. verbi, punteggiatura...).

![alt text](images/experts.png)

- Quando l'input scorre attraverso questo expert layer, uno o pi√π esperti sono selezionati e questi processeranno gli inputs, notare che si attivano solo gli esperti selezionati, gli altri rimangono disattivi. Questo viene chiamato **sparse model**, dato che solo un sottoinsieme di esperti vengono attivati in un certo istante.

![alt text](images/experts_1.png)

- Con questo **sparse model** spesso ci riferiamo come **MoE layer**.

- Il MoE layer quindi consiste di uno o pi√π esperti ciascuno dei quali √® una rete neurale feed-forward, questo MoE layer prende in input i dati e seleziona un esperto che √® pi√π adatto per il dato di input per generare l'output.  

- Come facciamo a sapere quali inputs dovrebbero andare a quale esperto?  

- Qu√¨ entra in gioco il **Router**.

---

## üì° Il router

- Il compito principale del **Router** √® quello di scegliere quale input deve andare verso quale esperto.

- Proprio come gli esperti il Router √® una rete neurale feedforward FFN, ma mlto pi√π piccola dato che deve solo instradare gli inputs, che assegna una **probabilit√†** a ciascun expert, che indica quanto un esperto √® adatto per tale input in particolare.

- Dopo aver creato i punteggi di probabilit√†, uno per ciascun esperto, il router decide **quali esperti attivare** in base a questi punteggi di probabilit√† (es. massimo punteggio o media pesata).
- Pu√≤ selezionare:
  - Un singolo expert
  - Pi√π esperti ‚Üí aggregazione con **media pesata** dei loro output. Gli esperti con le probabilit√† pi√π alte, date dal router, finiranno per avre maggiore voce in capitolo nel output finale.

![alt text](images/experts_2.png)


---

## ‚úÖ Vantaggi del MoE

I parametri di un modello che usa un MoE (Router + experts) possono trovarsi in sostanza in 5 posti diversi:

### üßÆ Parametri in un modello MoE

Le componenti che contengono parametri:
1. Input Embeddings
2. Self-Attention
3. Router
4. Experts
5. Output Embeddings

#### ‚úÖ Parametri **sparsi**:
- Quando carichiamo un modello dobbiamo caricare tutti i parametri.

![alt text](images/experts_3.png)

- Solo **una parte √® usata** durante l‚Äôinferenza


#### ‚úÖ Parametri **attivi**:
- Quelli **realmente utilizzati** per elaborare l'input.

- Con MoE non tutti gli esperti vengono utilizzati. Quindi, sebbene abbiamo bisogno di caricare tutti i parametri del modello, solo un sottoinsieme di essi viene utilizzato (parametri di uno o pi√π esperti) durante l'inferenza.

- Come risultato, la quantit√† di memoria necessaria per caricare l'intero modello √® relativamente alta per via dei molteplici esperti. Tuttavia, la memoria necessaria durante l'inferenza √® comparativamente bassa dato che non tutti gli esperti vengono utilizzati.

![alt text](images/experts_4.png)

---

## üìä Esempio: modello Mixtral 8x7B

- √à un MoE model che, comeil suo nome suggerisce, ha 8 experts, ciascuno con **~7 miliardi** di parametri.

- I parametri condivisi di Mixtral sono quelli che vengono sempre usati sia quando carichiamo il modello che quando si fa l'inferenza.  

- La maggior parte dei parametri si trovano nel meccanismo di attenzione con pi√π di un billion di parametri.

- **Router**: ha solo **32.000** parametri

![alt text](images/mixtral.png)

- Mixtral ha 8 esperti ciascuno dei quali ha in effetti ha 5.6 Billion di parametri e non i 7 Billion suggeriti dal nome.
- Assieme gli esperti fanno un Totale parametri pari a: 8 * 5.6 Billion = **~46 miliardi**. Questi dunque occupano il maggior m√¨numero di parametri del modello.

![alt text](images/mixtral_1.png)

- Dunque quando carichiamo Mixtral dobbiamo caricare tutti i 46.7 Billion di parametri.

- Eseguendo il modello e performando l'inferenza richiede i parametri condivisi.
- Tuttavia, Mixtral seleziona solo **2 experts usati per volta** ‚Üí riducendo di molto i parametri usati (12.8 Billion) durante l‚Äôinferenza.

![alt text](images/mixtral_2.png)


- Sebbene, il numero dei paramentri possa sembrare enorme, durante l'inferenza, il modello in realt√† ne usa molti di meno impiegando meno risorse computazionali.

- Questo rende i modelli MoE eccellenti quando gli eseguiamo in produzione 

---

## Pros della architettura MoE

| Aspetto | Vantaggio | Svantaggio
|--------|-----------|-------------|
| Memoria VRAM | ‚úÖ Usa meno VRAM durante l'inferenza perch√© pochi esperti attivi | ‚ùå Usa molta VRAM per caricare l'intero modello
| Efficienza | ‚úÖ Riduzione della ridondanza computazionale | 
| Prestazioni | ‚úÖ Migliori rispetto ai modelli densi tradizionali | ‚ùå Overfitting quando si usa un singolo esperto
| Flessibilit√† | ‚úÖ Gli esperti possono essere modulati e aggiornati |

## ‚ö†Ô∏è Svantaggi e sfide

- üö® **Overfitting** su un singolo expert ‚Üí serve bilanciamento attento
- üß† Architettura pi√π **complessa da addestrare**
- üß± Serve **pi√π memoria** per caricare il modello completo

---

## üîÑ MoE oltre i Transformer

- Il layer MoE agisce **solo sul feed-forward** layer
- Pu√≤ essere applicato anche a modelli **non Transformer**
  - Es: **Mamba**, **Jamba**
- Applicabile **in tutto il panorama LLM**, non solo nei decoder

---

## üìå Conclusione

Il Mixture of Experts √® una potente evoluzione architetturale che permette:
- Scalabilit√†
- Flessibilit√†
- Maggiore efficienza durante l‚Äôinferenza
